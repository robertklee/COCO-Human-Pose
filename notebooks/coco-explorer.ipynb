{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"coco-explorer.ipynb","provenance":[],"collapsed_sections":["D7Nn-niY7NE2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WCMkvELD6F0a"},"source":["Run this to mount your personal google drive.\n","Ensure you have COCO-Human-Pose repo cloned into your drive at the path on line 6."]},{"cell_type":"code","metadata":{"id":"PR05EklpFe0k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615576693493,"user_tz":480,"elapsed":20759,"user":{"displayName":"Julian Rocha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0QrES-WzNCHbIVd-DJKGaCtIC3OO5amuVSCFOPw=s64","userId":"17506259248702060718"}},"outputId":"8136985e-0d4a-458d-f34f-88bbab3591cc"},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# Switch directories\n","%cd /content/gdrive/MyDrive/Colab\\ Data/COCO-Human-Pose\n","%ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Data/COCO-Human-Pose\n","coco_df.py               hourglass_blocks.py  \u001b[0m\u001b[01;34moutput\u001b[0m/           \u001b[01;34msubmodules\u001b[0m/\n","constants.py             hourglass.py         \u001b[01;34m__pycache__\u001b[0m/      test.py\n","\u001b[01;34mdata\u001b[0m/                    LICENSE              README.md         train.py\n","data_generator.py        \u001b[01;34mlogs\u001b[0m/                \u001b[01;34mreport\u001b[0m/\n","data_generator_tests.py  \u001b[01;34mmodels\u001b[0m/              requirements.txt\n","evaluation.py            \u001b[01;34mnotebooks\u001b[0m/           \u001b[01;34mscripts\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SXT_WNmXwoy0"},"source":["!ls /content/gdrive/MyDrive/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJQO-WWrzIug"},"source":["Run this to load coco train/val keypoint annotations into an in-mem dataframe which can be queried. Ensure the paths make sense. It also this to declares a function that displays Image w/ annotations by file name"]},{"cell_type":"code","metadata":{"id":"uIF0HJhXur9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615577250768,"user_tz":480,"elapsed":27050,"user":{"displayName":"Julian Rocha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0QrES-WzNCHbIVd-DJKGaCtIC3OO5amuVSCFOPw=s64","userId":"17506259248702060718"}},"outputId":"099e810e-e2af-4323-b5e8-9af515699363"},"source":["# Filename is a globally unique identifier among all train/val/test splits\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","import pylab\n","import coco_df\n","from pycocotools.coco import COCO\n","from matplotlib.patches import Rectangle\n","pylab.rcParams['figure.figsize'] = (30.0, 30.0)\n","\n","train_annot_path = '/content/gdrive/MyDrive/COCO/2017/annotations/person_keypoints_train2017.json'\n","val_annot_path = '/content/gdrive/MyDrive/COCO/2017/annotations/person_keypoints_val2017.json'\n","train_coco = COCO(train_annot_path) # load annotations for training set\n","val_coco = COCO(val_annot_path) # load annotations for validation set\n","\n","df = coco_df.get_df(train_annot_path,val_annot_path)\n","\n","def display_img(image_file_name):\n","  # Determine if img exists and if it is in train or val set\n","  img_df_rows = df.loc[df['path'] == 'train2017/'+image_file_name]\n","  if len(img_df_rows) == 0:\n","    img_df_rows = df.loc[df['path'] == 'val2017/'+image_file_name]\n","    if len(img_df_rows) == 0:\n","      print(\"Image with filename: \" + image_file_name + \" does not exist.\")\n","      return\n","  \n","  coco = train_coco if img_df_rows['source'].iloc[0] == 0 else val_coco\n","\n","  # Get img id from file name\n","  imgId = img_df_rows['src_set_image_id'].iloc[0]\n","  img = coco.imgs[imgId]\n","  I = io.imread(img['coco_url']) # load image from URL (no need to store image locally)\n","\n","  # load and display keypoints annotations\n","  plt.subplot(1,2,1)\n","  plt.imshow(I)\n","  plt.axis('off')\n","\n","  plt.subplot(1,2,2)\n","  plt.imshow(I)\n","  annIds = coco.getAnnIds(imgIds=[imgId])\n","  anns = coco.loadAnns(annIds)\n","  bbox= anns[0]['bbox']\n","  plt.gca().add_patch(Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],\n","                  edgecolor='red',\n","                  facecolor='none',\n","                  lw=4))\n","  coco.showAnns(anns)\n","  plt.show()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=8.84s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=2.78s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=8.72s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.28s)\n","creating index...\n","index created!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dfNASRqc7Cfb"},"source":["Example of using the display_img function"]},{"cell_type":"code","metadata":{"id":"AqHJDitP7BSH"},"source":["image_file_name = '000000163171.jpg'\n","display_img(image_file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cz2UysAqqvS0","colab":{"base_uri":"https://localhost:8080/","height":949},"executionInfo":{"status":"ok","timestamp":1615577257428,"user_tz":480,"elapsed":568,"user":{"displayName":"Julian Rocha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0QrES-WzNCHbIVd-DJKGaCtIC3OO5amuVSCFOPw=s64","userId":"17506259248702060718"}},"outputId":"9a1d8980-b454-4897-d8a4-0d738a6fc634"},"source":["# see the contents of the coco-df\n","df.loc[df['is_crowd'] == 0]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>src_set_image_id</th>\n","      <th>coco_url</th>\n","      <th>path</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>ann_id</th>\n","      <th>is_crowd</th>\n","      <th>bbox</th>\n","      <th>bbox_area</th>\n","      <th>area</th>\n","      <th>num_keypoints</th>\n","      <th>keypoints</th>\n","      <th>segmentation</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>36</td>\n","      <td>http://images.cocodataset.org/train2017/000000...</td>\n","      <td>train2017/000000000036.jpg</td>\n","      <td>481</td>\n","      <td>640</td>\n","      <td>453991</td>\n","      <td>0</td>\n","      <td>[167.58, 162.89, 310.61, 465.19]</td>\n","      <td>144492.6659</td>\n","      <td>86145.29710</td>\n","      <td>13</td>\n","      <td>[250, 244, 2, 265, 223, 2, 235, 235, 2, 309, 2...</td>\n","      <td>[[345.28, 220.68, 348.17, 269.8, 355.4, 307.36...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>49</td>\n","      <td>http://images.cocodataset.org/train2017/000000...</td>\n","      <td>train2017/000000000049.jpg</td>\n","      <td>381</td>\n","      <td>500</td>\n","      <td>198565</td>\n","      <td>0</td>\n","      <td>[203.39, 260.43, 65.69, 73.04]</td>\n","      <td>4797.9976</td>\n","      <td>1754.81755</td>\n","      <td>13</td>\n","      <td>[251, 276, 2, 253, 275, 2, 250, 274, 2, 259, 2...</td>\n","      <td>[[260.74, 281.51, 263.68, 283.47, 263.68, 290....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>49</td>\n","      <td>http://images.cocodataset.org/train2017/000000...</td>\n","      <td>train2017/000000000049.jpg</td>\n","      <td>381</td>\n","      <td>500</td>\n","      <td>254537</td>\n","      <td>0</td>\n","      <td>[118.43, 261.32, 56.91, 62.93]</td>\n","      <td>3581.3463</td>\n","      <td>1349.98230</td>\n","      <td>11</td>\n","      <td>[158, 270, 1, 160, 269, 2, 157, 269, 1, 167, 2...</td>\n","      <td>[[153.35, 277.78, 156.08, 275.73, 158.81, 270....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>49</td>\n","      <td>http://images.cocodataset.org/train2017/000000...</td>\n","      <td>train2017/000000000049.jpg</td>\n","      <td>381</td>\n","      <td>500</td>\n","      <td>1211660</td>\n","      <td>0</td>\n","      <td>[119.34, 334.21, 11.94, 33.57]</td>\n","      <td>400.8258</td>\n","      <td>250.81990</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[125.66, 367.36, 127.77, 367.36, 128.75, 367....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>49</td>\n","      <td>http://images.cocodataset.org/train2017/000000...</td>\n","      <td>train2017/000000000049.jpg</td>\n","      <td>381</td>\n","      <td>500</td>\n","      <td>2007474</td>\n","      <td>0</td>\n","      <td>[284.34, 333.4, 10.88, 27.43]</td>\n","      <td>298.4384</td>\n","      <td>162.90005</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[289.7, 357.59, 291.97, 358.89, 293.43, 360.8...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>273464</th>\n","      <td>581357</td>\n","      <td>http://images.cocodataset.org/val2017/00000058...</td>\n","      <td>val2017/000000581357.jpg</td>\n","      <td>612</td>\n","      <td>612</td>\n","      <td>1691804</td>\n","      <td>0</td>\n","      <td>[180.89, 440.51, 21.1, 38.36]</td>\n","      <td>809.3960</td>\n","      <td>371.49395</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[184, 440.75, 180.89, 457.77, 184.72, 463.53,...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>273465</th>\n","      <td>581357</td>\n","      <td>http://images.cocodataset.org/val2017/00000058...</td>\n","      <td>val2017/000000581357.jpg</td>\n","      <td>612</td>\n","      <td>612</td>\n","      <td>1698852</td>\n","      <td>0</td>\n","      <td>[249.22, 419.65, 13.2, 24.53]</td>\n","      <td>323.7960</td>\n","      <td>169.55635</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[251.96, 444.18, 249.22, 442.46, 250.42, 438....</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>273466</th>\n","      <td>581357</td>\n","      <td>http://images.cocodataset.org/val2017/00000058...</td>\n","      <td>val2017/000000581357.jpg</td>\n","      <td>612</td>\n","      <td>612</td>\n","      <td>1726027</td>\n","      <td>0</td>\n","      <td>[211.18, 420.26, 23.38, 40.03]</td>\n","      <td>935.9014</td>\n","      <td>542.16740</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[212.78, 430.08, 211.18, 435.02, 212.49, 435....</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>273467</th>\n","      <td>581357</td>\n","      <td>http://images.cocodataset.org/val2017/00000058...</td>\n","      <td>val2017/000000581357.jpg</td>\n","      <td>612</td>\n","      <td>612</td>\n","      <td>1748315</td>\n","      <td>0</td>\n","      <td>[230.28, 420.86, 19.02, 37.35]</td>\n","      <td>710.3970</td>\n","      <td>371.93055</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[242.81, 427.82, 245.12, 436.17, 246.05, 440....</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>273468</th>\n","      <td>581357</td>\n","      <td>http://images.cocodataset.org/val2017/00000058...</td>\n","      <td>val2017/000000581357.jpg</td>\n","      <td>612</td>\n","      <td>612</td>\n","      <td>2149057</td>\n","      <td>0</td>\n","      <td>[19.53, 261.51, 29.31, 76.62]</td>\n","      <td>2245.7322</td>\n","      <td>764.53180</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>[[20.99, 334.47, 22.82, 322.61, 41.53, 338.13,...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>268030 rows Ã— 14 columns</p>\n","</div>"],"text/plain":["        src_set_image_id  ... source\n","0                     36  ...      0\n","1                     49  ...      0\n","2                     49  ...      0\n","3                     49  ...      0\n","4                     49  ...      0\n","...                  ...  ...    ...\n","273464            581357  ...      1\n","273465            581357  ...      1\n","273466            581357  ...      1\n","273467            581357  ...      1\n","273468            581357  ...      1\n","\n","[268030 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"D7Nn-niY7NE2"},"source":["# All code blocks below are example queries on the coco df."]},{"cell_type":"code","metadata":{"id":"HOZ3LJyj8qPv"},"source":["# Number of images in train + val\n","len(coco_df.groupby('path',as_index=False).size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPBARmLJpZqT"},"source":["# Breakdown of num images with a given num annotations\n","coco_df.groupby('path',as_index=False).size().rename(columns = {'size': 'num_annotations'}).groupby('num_annotations',as_index=False).size().rename(columns={'size':'num_images'}).sort_values(by='num_images',ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ELObBd27yWl"},"source":["# images with crowds\n","coco_df.loc[coco_df['is_crowd'] == 1].groupby('path', as_index=False).size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZoDvEkW9MEB"},"source":["# Breakdown of num of annotations with a given num keypoints\n","coco_df.groupby('num_keypoints',as_index=False).size().sort_values(by='size',ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7oP0B3o9_Vm"},"source":["# Annotations with X keypoints\n","coco_df.loc[coco_df['num_keypoints'] == 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_TWZQ7IDcLA"},"source":["# The image with the most people (20)\n","coco_df.groupby('path',as_index=False).size().loc[coco_df.groupby('path',as_index=False).size()['size'] == 20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YB1Qp9_GUzc"},"source":["# how many images have an annotation with x or more keypoints?\n","# how many of those images have only one annotation?\n","\n","# num images where best annotation has X keypoints\n","df = coco_df.groupby('path',as_index=False).max('num_keypoints').groupby('num_keypoints',as_index=False).size()\n","\n","# increase sizes so that sum represents all images with X or MORE keypoints\n","for i, row in df.iterrows():\n","  row['size'] += sum(df.loc[df['num_keypoints'] > row['num_keypoints']]['size'])\n","\n","df.rename(columns={'size':'num_imgs'},inplace=True)\n","\n","anns_per_img = coco_df.groupby('path',as_index=False).size()\n","imgs_w_one_ann = anns_per_img.loc[anns_per_img['size'] == 1]\n","imgs_w_one_ann = pd.merge(imgs_w_one_ann,coco_df)\n","df2 = imgs_w_one_ann.groupby('num_keypoints',as_index=False).size()\n","\n","# increase sizes so that sum represents all images with X or MORE keypoints\n","for i, row in df2.iterrows():\n","  row['size'] += sum(df2.loc[df['num_keypoints'] > row['num_keypoints']]['size'])\n","\n","df2.rename(columns={'size':'num_imgs_with_only_one_ann'},inplace=True)\n","\n","pd.merge(df,df2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sCwWsvnooNz"},"source":["## major questions ##\n","# how will model handle images with more than one person?\n","# how will model handle images with half a person/person far away? (will num outputted keypoints be constant or dynamic?)"],"execution_count":null,"outputs":[]}]}